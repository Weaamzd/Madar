# -*- coding: utf-8 -*-
"""Project: KPI AI Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zTVO6eqb9W0GMJCj-txfaW67qQLbAixK

# Project: **KPI AI Model**.

###  Notes

- The dataset is synthetic and used only for AI training and research.
- Results support decision-making.
- Model accuracy depends on data quality.

##Introduction

###  Project Introduction

This project aims to build an AI-powered system for monitoring, analyzing, and improving organizational performance through Key Performance Indicators (KPIs). The system goes beyond simply displaying KPI values: it evaluates their current status, predicts future risks, detects unusual behavior, explains root causes, and recommends corrective or strategic actions.

To ensure a clear and scalable implementation, the work will be organized into four main phases:


1. **Data Preparation**  
   Cleaning the dataset, generating useful features, and assigning KPI status labels based on performance vs target.

2. **Prediction Models**  
   Using supervised machine learning to classify the current KPI status and predict whether the KPI will fall below its target in the next period.

3. **Anomaly Detection & Explainability**  
   Detecting abnormal spikes or drops, and using Explainable AI (SHAP) to show which operational factors had the strongest impact.

4. **Generative AI Recommendations**  
   Using a language model (LLM) to turn numeric outputs into readable insights ‚Äî explaining reasons, proposing improvement actions, and recommending new targets when performance is high.

This roadmap grows the system from simple KPI monitoring to smart, predictive, and fully decision-supportive insights.

##About Dataset

##  KPI Dataset Overview

This dataset contains **synthetically generated historical Key Performance Indicators (KPIs)** for a governmental entity.  
The data is designed specifically for **AI model training, experimentation**, and does **not** represent real governmental performance.

- **Total records:** 2,875 synthetic KPI rows  
- **Time coverage:** January 2018 to December 2024  
- **Time horizons covered:** monthly, quarterly, yearly, and 5-year cumulative project KPIs  
- **Granularity:** Monthly-aligned observations to standardize input for modeling  
- **Primary use case:** Training predictive AI models for KPI performance forecasting, anomaly detection, and strategic performance optimization  

Each row represents the performance value of a specific KPI during a given period, along with operational and organizational factors that may influence its performance (such as budget, number of employees, workload, and region).

>  Note:  
> All values in this dataset are **artificially generated (synthetic data)** and are intended **only** for research, experimentation, and demonstration of AI techniques.



### 1Ô∏è KPI Types and Period Formats

- **Monthly KPIs**
  - Period format: `YYYY-MM` (e.g., `2020-03`)
  - Identifiers: `KPI_M_001` to `KPI_M_010`

- **Quarterly KPIs**
  - Period format: `YYYY-Q1`, `YYYY-Q2`, `YYYY-Q3`, `YYYY-Q4` (e.g., `2021-Q3`)
  - Identifiers: `KPI_Q_001` to `KPI_Q_005`

- **Yearly KPIs**
  - Period format: `YYYY` (e.g., `2022`)
  - Identifiers: `KPI_Y_001` to `KPI_Y_003`

- **Cumulative 5-Year Project KPIs**
  - Period format: `YYYY-C1`, `YYYY-C2`, ‚Ä¶, `YYYY-C5`  
    (e.g., `2020-C1` = Year 1 of a 5-year project)
  - Identifiers: `KPI_P_001` to `KPI_P_005`
  - For this category, `value` and `target` represent cumulative project completion (e.g., 20%, 40%, ‚Ä¶, 100%).

---

### 2Ô∏è Raw Input Fields

The dataset includes the following core fields:

- `kpi_id` ‚Üí KPI code (monthly, quarterly, yearly, or cumulative project).
- `period` ‚Üí Time period according to the KPI type.
- `value` ‚Üí Actual achieved value of the KPI for the given period.
- `target` ‚Üí Planned target value for the same period.
- `unit` ‚Üí Government administration or organizational unit.
- `department` ‚Üí Sector or division.
- `region` ‚Üí Geographic region (e.g., Riyadh, Jeddah, Dammam, Makkah).
- `num_employees` ‚Üí Number of employees involved in the KPI scope.
- `budget` ‚Üí Budget consumed during the period.
- `num_cases` ‚Üí Number of cases/transactions/projects related to the KPI.

**Period-related helper fields:**

- `month_number`  
  - Monthly KPIs: 1‚Äì12  
  - Quarterly KPIs: representative month (e.g., Q1 = 3, Q2 = 6, Q3 = 9, Q4 = 12)  
  - Yearly / cumulative projects: 12 (end-of-year evaluation).

- `is_season`  
  - Monthly: 1 in high-season months (e.g., 3, 4, 11, 12), otherwise 0.  
  - Quarterly: 1 in Q1 and Q4, otherwise 0.  
  - Yearly / cumulative projects: always 0.

---

### 3Ô∏è Engineered Features for AI Modeling

The following features are pre-computed for each KPI over time and can be used directly in AI models:

- `diff`  
  = `value - target`  
  ‚Üí Absolute difference between actual performance and target  
  (positive = above target, negative = below target).

- `diff_pct`  
  = `(value - target) / target`  
  ‚Üí Percentage deviation from target  
  (e.g., `0.10` = 10% above target).

- `rolling_avg_3`  
  ‚Üí 3-period moving average (months/quarters/years depending on KPI type), capturing trend behavior.

- `rolling_std_3`  
  ‚Üí 3-period moving standard deviation, reflecting performance volatility  
  (high = unstable performance, low = stable performance).

This dataset is designed for training AI models to forecast KPI performance, detect early deviations, and support data-driven decision making.

###  Tools Used
- **Pandas** ‚Äî Data processing, reading CSV/Excel files, and cleaning columns.
- **NumPy** ‚Äî Fast mathematical and matrix operations.
- **Matplotlib** ‚Äî Basic visualizations (Histogram, Bar Chart, Line Plot).
- **Seaborn** ‚Äî Advanced visualizations (Heatmap, Boxplot) with improved styling.
- **Scikit-Learn** ‚Äî Core ML utilities:
  - Train/Test split
  - Evaluation metrics
  - One-Hot Encoding
  - RandomizedSearchCV for hyperparameter tuning
- **XGBoost** ‚Äî High-performance algorithm for tabular data and multi-class classification.
- **Google Colab** ‚Äî Free cloud environment to run Python and train ML models easily.

---

###  Concepts Used
- **Supervised Learning** ‚Äî Because we have input features and labeled outputs.
- **Multi-Class Classification** ‚Äî Predicting one of four KPI status classes.
- **Feature Engineering** ‚Äî Creating helpful features (diff, diff_pct, rolling_avg_3).
- **One-Hot Encoding** ‚Äî Converting text/categorical fields into numeric values.
- **Train/Test Split** ‚Äî Testing the model on unseen data.
- **Cross-Validation** ‚Äî Improving reliability by evaluating on multiple splits.
- **Hyperparameter Tuning** ‚Äî Optimizing XGBoost settings for best performance.
- **EDA (Exploratory Data Analysis)** ‚Äî Visual study of patterns before training.

---

###  Evaluation Metrics
- **Accuracy** ‚Äî Overall performance.
- **Precision** ‚Äî Correctness of positive predictions.
- **Recall** ‚Äî Ability to detect true positive cases.
- **F1-Score** ‚Äî Balanced measure between Precision and Recall.
- **Confusion Matrix** ‚Äî Visual breakdown of correct and incorrect classifications.

---

###  Why XGBoost?
- Built for **tabular datasets** like KPI data.
- Delivers **high accuracy** compared to traditional ML algorithms.
- Handles **many engineered features** effectively.
- Captures **non-linear relationships** between variables.
- Offers strong interpretability through **Feature Importance** and **SHAP values**.

## Installing and importing dependenciesa
"""

!pip install xgboost openpyxl seaborn

"""Let‚Äôs start by importing all the necessary libraries."""

!pip install arabic-reshaper python-bidi

!pip install imbalanced-learn

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import arabic_reshaper
from bidi.algorithm import get_display
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer, f1_score
from sklearn.preprocessing import RobustScaler
from imblearn.over_sampling import SMOTE

all_fonts = fm.findSystemFonts()

arabic_candidates = [
    f for f in all_fonts
    if any(word in f.lower() for word in ['arab', 'kacst', 'noto'])
]

print("Arabic-like fonts found:")
for f in arabic_candidates[:10]:
    print(" -", f)

if arabic_candidates:
    font_path = arabic_candidates[0]
    font_prop = fm.FontProperties(fname=font_path)
    plt.rcParams['font.family'] = font_prop.get_name()
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['font.size'] = 14
    print("\n Using font:", font_prop.get_name())
else:
    print(" No Arabic font found. Please upload a TTF Arabic font file and use its path.")

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 14
plt.rcParams['axes.unicode_minus'] = False

def ar(text: str) -> str:
    reshaped = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped)
    return bidi_text

from google.colab import drive
drive.mount('/content/drive')

"""## Import the dataset

A. Connect Google Colab to Google Drive
"""

#uploaded = files.upload()
file_name = "/content/kpi_data_mixed_frequencies_gov_2875rows.xlsx"
df = pd.read_excel(file_name)

"""## Exploratory Data Analysis (EDA)

1. Dataset Structure
"""

print("Shape:", df.shape)

print("\nColumns:")
print(df.columns.tolist())

print("\nData Types:")
print(df.dtypes)

print("\nHead:")
display(df.head())

print("\nTail:")
display(df.tail())

"""2. Missing Values"""

print("\nMissing Values per Column:")
print(df.isnull().sum())

print("\nMissing Percentage per Column:")
print((df.isnull().sum() / len(df) * 100).round(2))

"""3. Unique Values"""

print("\nUnique Values:")
print(df.nunique())

"""4. Numerical Stats"""

print("\nStatistical Summary for Numerical Columns:")
display(df.describe())

"""5. Categorical Stats"""

categorical_cols = df.select_dtypes(include=['object'])
print("\nCategorical Columns:")
print(categorical_cols.columns.tolist())

for col in categorical_cols.columns:
    print(f"\n--- {col} ---")
    print(df[col].value_counts().head(10))

"""6. Correlation Matrix"""

numeric_df = df.select_dtypes(include=['int64', 'float64'])

#plt.figure(figsize=(14, 6))
#sns.heatmap(numeric_df.corr(), annot=False, cmap='Blues')
#plt.title("Correlation Heatmap ‚Äî Numerical Columns Only")
#plt.show()

plt.figure(figsize=(14, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='Blues', fmt=".2f")
plt.title("Correlation Heatmap ‚Äî With Values")
plt.show()

"""Visual Summary of the Dataset ‚Äî Comprehensive Statistical Charts"""

sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (10, 5)

"""###  What is a Histogram?

A **Histogram** is a chart that shows how a numeric value is distributed in the dataset.  
Instead of listing each value, it groups numbers into ranges and shows how many records fall inside each range.

Simply:
- It tells us whether most values are small, medium, or large.
- It helps us see where the data is concentrated and how it spreads.

‚û° A histogram helps us quickly understand the pattern of a numeric column.

"""

# 1Ô∏è Histogram
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

cols_per_row = 3
rows = (len(numeric_cols) // cols_per_row) + (1 if len(numeric_cols) % cols_per_row else 0)

fig, axes = plt.subplots(rows, cols_per_row, figsize=(18, 5 * rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.histplot(df[col], kde=True, ax=axes[i])
    axes[i].set_title(f"Histogram ‚Äî {col}")

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""###  Boxplot

A **Boxplot** is a chart that shows how a numeric value is spread out in the dataset and helps identify unusual records.

It shows:
- The middle (typical) value.
- The range of most values.
- Whether the numbers are close together or very spread out.
- **Outliers** ‚Äî values that are much higher or lower than the rest.

‚û° A Boxplot helps quickly detect stability vs. unusual or unexpected values in a numeric column.

"""

# 2Ô∏è Boxplot
numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

cols_per_row = 3
rows = (len(numeric_cols) // cols_per_row) + (1 if len(numeric_cols) % cols_per_row else 0)

fig, axes = plt.subplots(rows, cols_per_row, figsize=(18, 6 * rows))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.boxplot(ax=axes[i], x=df[col])
    axes[i].set_title(f"Boxplot ‚Äî {col}")

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""### Category Charts

- **Region** ‚Üí Shows which region has the highest number of KPI records.
- **KPI_Category** ‚Üí Shows which KPI category appears most frequently.
- **Organization / Department** ‚Üí Shows which departments are the most active or have the highest participation.

These charts help quickly understand which areas contribute the most to the dataset and where performance analysis should focus.

"""

categorical_cols = df.select_dtypes(include=['object']).columns

n = len(categorical_cols)
cols_per_row = 3
rows = (n // cols_per_row) + (1 if n % cols_per_row else 0)

fig, axes = plt.subplots(rows, cols_per_row, figsize=(20, 6 * rows))
axes = axes.flatten()

for i, col in enumerate(categorical_cols):
    values = df[col].value_counts().head(15)
    labels = [ar(str(x)) for x in values.index]

    axes[i].bar(labels, values.values)
    axes[i].set_title(ar(f"ÿ£ÿπŸÑŸâ 15 ŸÅÿ¶ÿ© ‚Äî {col}"))
    axes[i].tick_params(axis='x', rotation=35)

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

categorical_cols = df.select_dtypes(include=['object']).columns

cols_per_row = 3
rows = (len(categorical_cols) // cols_per_row) + (1 if len(categorical_cols) % cols_per_row else 0)

fig, axes = plt.subplots(rows, cols_per_row, figsize=(18, 6 * rows))
axes = axes.flatten()

for i, col in enumerate(categorical_cols):
    values = df[col].value_counts().head(10)
    labels = [ar(str(x)) for x in values.index]

    axes[i].pie(values.values, labels=labels, autopct='%1.1f%%')
    axes[i].set_title(ar(f"ÿßŸÑÿ±ÿ≥ŸÖ ÿßŸÑÿØÿßÿ¶ÿ±Ÿä ‚Äî {col}"))


for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# 5 Pairplot
sns.pairplot(df[numeric_cols].sample(min(200, len(df))))
plt.show()

"""Summary"""

numeric_cols = [
    "value",
    "target",
    "diff",
    "diff_pct",
    "rolling_avg_3",
    "rolling_std_3",
    "num_employees",
    "budget",
    "num_cases",
    "month_number",
    "is_season"
]

categorical_cols = ["unit", "department", "region"]
label_col = "current_status"

print(" Column Overview:")
print("Numeric columns:", numeric_cols)
print("Categorical columns:", categorical_cols)
print("Label column:", label_col)

print("\nSummary:")
print("Rows:", df.shape[0])
print("Columns:", df.shape[1])
print("Missing Cells:", df.isnull().sum().sum())
print("Numeric Columns:", len(df.select_dtypes(include=['int64','float64']).columns))
print("Categorical Columns:", len(df.select_dtypes(include=['object']).columns))

"""##  Preprocessing"""

def label_status(diff_pct):
    if diff_pct < -0.20:
        return "ÿ≠ÿ±ÿ¨"
    elif diff_pct < -0.05:
        return "ŸÅŸä ÿÆÿ∑ÿ±"
    elif diff_pct <= 0.05:
        return "ÿ∑ÿ®ŸäÿπŸä"
    else:
        return "ŸÖÿ™ŸÅŸàŸÇ"

df["current_status"] = df["diff_pct"].apply(label_status)

df["current_status"].value_counts()

numeric_cols = [
    "value",
    "target",
    "diff",
    "diff_pct",
    "rolling_avg_3",
    "rolling_std_3",
    "num_employees",
    "budget",
    "num_cases",
    "month_number",
    "is_season"
]

categorical_cols = ["unit", "department", "region"]

label_col = "current_status"

print(" Column Overview:")
print("-------------------------------------------------")
print(f" Number of numerical columns: {len(numeric_cols)}")
print("‚û° Numerical Columns:", numeric_cols)
print()
print(f" Number of categorical columns: {len(categorical_cols)}")
print("‚û° Categorical Columns:", categorical_cols)
print()
print(f" Target / Label Column:", label_col)
print("-------------------------------------------------")
print(" Columns have been grouped successfully and are ready for preprocessing / visualization.")

df_work = df.copy()

before_dup = df_work.shape[0]
df_work = df_work.drop_duplicates()
after_dup = df_work.shape[0]

print("\n Basic Cleaning")
print(f"Removed {before_dup - after_dup} duplicate rows.")

print("\nMissing values BEFORE handling:")
print(df_work[numeric_cols + categorical_cols + [label_col]].isnull().sum())

df_work = df_work.dropna(subset=[label_col])


for col in numeric_cols:
    df_work[col] = df_work[col].fillna(df_work[col].median())


for col in categorical_cols:
    df_work[col] = df_work[col].fillna("Unknown")

print("\nMissing values AFTER handling:")
print(df_work[numeric_cols + categorical_cols + [label_col]].isnull().sum())
print(" Missing values handled.")

df_clean = df_work.copy()

print("\nOutlier Treatment (IQR + clipping)")
print("--------------------------------------------------")

for col in numeric_cols:
    Q1 = df_clean[col].quantile(0.25)
    Q3 = df_clean[col].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    before = df_clean[col].copy()
    df_clean[col] = np.clip(df_clean[col], lower, upper)
    changed = (before != df_clean[col]).sum()

    print(f" Column '{col}': {changed} values clipped.")

print("--------------------------------------------------")
print("Outlier handling completed.")

from scipy.stats import skew

skewness = df_clean[numeric_cols].apply(lambda x: skew(x.dropna()))
skewed_cols = skewness[abs(skewness) > 1].index.tolist()

print("\n Skewness Analysis")
print("--------------------------------------------------")
print("Skewness values:")
print(skewness)
print("--------------------------------------------------")
print(f" Columns with strong skew (|skew| > 1): {skewed_cols if skewed_cols else 'None'}")

df_log = df_clean.copy()

print("\n Log Transformation")
print("--------------------------------------------------")
for col in skewed_cols:
    col_min = df_log[col].min()
    shift = 1 - col_min if col_min <= 0 else 0
    df_log[col] = np.log1p(df_log[col] + shift)
    print(f" Column '{col}': log1p applied (shift = {shift}).")
print("--------------------------------------------------")
print(" Log transformation completed.")

df_scaled = df_log.copy()

scaler = RobustScaler()
df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])

print("\nScaling Report")
print("--------------------------------------------------")
print("üîß Method: RobustScaler()")
print(f"Scaled columns ({len(numeric_cols)}):")
for col in numeric_cols:
    print(f"   ‚Ä¢ {col}")
print("--------------------------------------------------")
print("Scaling completed.")

df_final = pd.get_dummies(df_scaled, columns=categorical_cols, drop_first=True)

dummy_cols = [c for c in df_final.columns if any(prefix in c for prefix in categorical_cols)]
df_final[dummy_cols] = df_final[dummy_cols].astype(int)

print("\nOne-Hot Encoding Report")
print("--------------------------------------------------")
print("Encoded categorical columns:", categorical_cols)
print(f"Number of dummy columns: {len(dummy_cols)}")
print("Final dataset shape:", df_final.shape)
print("--------------------------------------------------")
print("Preview of dummy columns:")
display(df_final[dummy_cols].head())

"""## Splitting the Data

Split data into test and train
"""

X = df_final.drop(label_col, axis=1)
y = df_final[label_col]

print("\n Features/Target Shapes")
print("X shape:", X.shape)
print("y shape:", y.shape)

if y.dtype == 'object':
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)
    print("\n Label encoding applied on target column.")
    print("Class mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)

print("\n Train/Test Split")
print("X_train:", X_train.shape, " | X_test:", X_test.shape)
print("y_train:", y_train.shape, " | y_test:", y_test.shape)

obj_cols = X_train.select_dtypes(include=['object']).columns
print(" Object columns found in X_train:", list(obj_cols))


X_train = X_train.drop(columns=obj_cols)
X_test = X_test.drop(columns=obj_cols)

print(" Removed object columns. New shapes:")
print("X_train:", X_train.shape)
print("X_test:", X_test.shape)

model = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.9,
    colsample_bytree=0.9,
    eval_metric='mlogloss'
)


model.fit(X_train, y_train)
print("\n Model training completed!")

# ÿßŸÑÿ™ŸÜÿ®ÿ§
y_pred = model.predict(X_test)

"""#### Evaluating the Model"""

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ÿßŸÑÿ™ŸÇŸäŸäŸÖ
acc = accuracy_score(y_test, y_pred)
print("\n Evaluation Metrics")
print("Accuracy:", round(acc, 4))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix ‚Äî XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()